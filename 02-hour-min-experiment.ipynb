{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataframe from multiple csv files\n",
    "\n",
    "# get list of available csv files\n",
    "csv_files = glob.glob('./data/*.csv')\n",
    "\n",
    "# read each file and create final df\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    dataframes.append(temp_df)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # convert unix_train time to date time\n",
    "df.sort_values(by='Timestamp', inplace=True)  # sort by timestamp\n",
    "\n",
    "# n_feeatures = 3  # number of features used to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 44979 entries, 0 to 44978\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Number     44979 non-null  float64       \n",
      " 1   Timestamp  44979 non-null  datetime64[ns]\n",
      " 2   IP         44979 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Eng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_attr(row: pd.Series, part: str):\n",
    "    \"\"\"Return the specified part of the timestamp.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Dataframe row\n",
    "        part (str): Should be a valid `pandas.Timestamp` attribute\n",
    "\n",
    "    Returns:\n",
    "        any: Specified attribute of the timestamp\n",
    "    \"\"\"\n",
    "    return getattr(row, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time attributes and add to dataframe\n",
    "needed_time_attrs = [\"hour\", \"minute\"]\n",
    "\n",
    "for attr in needed_time_attrs:\n",
    "    df[attr] = df[\"Timestamp\"].apply(lambda x: get_time_attr(x, attr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_fraction = 0.8\n",
    "split_point = int(len(df) * train_fraction)\n",
    "\n",
    "df_train = df.iloc[:split_point]\n",
    "df_test = df.iloc[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw sequence data\n",
    "train_raw_seq = np.array(df_train[[\"Number\", \"hour\", \"minute\"]])\n",
    "n_features = train_raw_seq.shape[-1]  # Number of features being used\n",
    "n_look_back = 50  # Number of historical data points to learn for each result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look back sequences\n",
    "def split_sequence(sequence: np.ndarray, n_steps: int, n_features: int):\n",
    "    \"\"\"Make sub sequences of n length and the prediction value and return them\n",
    "\n",
    "    Args:\n",
    "        sequence (numpy.ndarray): The sequence of training data\n",
    "        n_steps (int): How many past data points to look at\n",
    "        n_features (int): Total number of features in given dataset\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        # find index of end of current learning slice\n",
    "        end_i = i + n_steps\n",
    "\n",
    "        # check if end is beyond the length of sequence\n",
    "        if end_i > len(sequence) - 1:  # -1 since last element needs to be the prediction value\n",
    "            break\n",
    "        else:\n",
    "            seq_x = sequence[i:end_i]  # training sequence\n",
    "            seq_y = sequence[end_i]  # prediction value\n",
    "\n",
    "            x.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "\n",
    "    # Reshape data\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    x = x.reshape(x.shape[0], x.shape[1], n_features)  # each internal array contains data of each timestep\n",
    "    y = y.reshape(-1, n_features)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train, test splits\n",
    "X_train, y_train = split_sequence(train_raw_seq, n_look_back, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "def train_model(X_train, y_train, n_features, activation_function, epoch_count=15, optimizer_algorithm='adam', loss_metric='mean_squared_logarithmic_error'):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation=activation_function, input_shape=(50, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer_algorithm, loss=loss_metric)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(X_train, y_train, epochs=epoch_count, verbose=3, validation_split=0.2)\n",
    "\n",
    "    # plot train and validation loss\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Model loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "    # return trained model object\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu\n",
    "model_1 = train_model(X_train, y_train, n_features, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "model_1_path = './models/02-hour_min-relu.keras'\n",
    "model_1.save(model_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "model_2 = train_model(X_train, y_train, n_features, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "model_2_path = './models/02-hour_min-sigmoid.keras'\n",
    "model_2.save(model_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluate function\n",
    "def evaluate_msle(model: tensorflow.keras.models.Sequential, preds: np.array, ground_truths: np.array):\n",
    "    m = tensorflow.keras.metrics.MeanSquaredLogarithmicError()\n",
    "    m.update_state(ground_truths, preds)\n",
    "    metric = m.result().numpy()\n",
    "    m.reset_state()\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models again\n",
    "model_1 = tensorflow.keras.models.load_model('model_1.keras')\n",
    "model_2 = tensorflow.keras.models.load_model('model_2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_raw_seq = np.array(df_test[[\"Number\", \"hour\", \"minute\"]])\n",
    "X_test, y_test = split_sequence(test_raw_seq, n_look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds_1 = model_1.predict(X_test)\n",
    "preds_2 = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 1 MSLE: \", evaluate_msle(model_1, preds_1, y_test))\n",
    "print(\"Model 2 MSLE: \", evaluate_msle(model_2, preds_2, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
